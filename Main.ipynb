{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c0dc3ec-1a10-43da-9b49-41e4b2971a1c",
   "metadata": {},
   "source": [
    "# Value-at-risk\n",
    "\n",
    "---\n",
    "\n",
    "VaR is a measure of portfolio risk. For instance, a 1% VaR of -5% means that there is a 1% chance of earning a portfolio return of less than -5%. Think of it as a (lower) percentile or quantile of a portfolio returns distribution, i.e., we are concerned about the tail risk — the small chance of losing a remarkably large portfolio value. Such a large loss is funded by our own funds, i.e., capital which is an expensive source of funding compared to other peoples’ funds, i.e., debt. Therefore the estimation of VaR and similar market risk management measures inform banks and insurance firms with regards to the levels of capital they need to hold in order to have a buffer against unexpected downturns — market risk.\n",
    "\n",
    "For our purpose, let us begin by fetching a data set of 5 stocks from Yahoo. The stocks are Apple, Google, Microsoft, Intel and Box. We use a daily frequency for our data for the year 2016. We use the stock's daily closing prices to compute the continuously compounded returns: $\\log\\left(\\frac{V_{t+1}}{V_{t}}\\right) = \\log(V_{t+1}) - \\log(V_{t})$.\n",
    "\n",
    "![](portfolio_returns.png)\n",
    "\n",
    "Let's estimate the expected returns vector, volatilities vector, correlation and variance-covariance matrices. The variance-covariance matrix is recovered from the estimated volatilities vector and correlation matrix: $\\Omega = C \\odot \\sigma \\sigma^{T}$ where $\\odot$ is the Hadamard product, $C \\in \\mathbb{R}^{5 \\times 5}$ and $\\sigma \\in \\mathbb{R}^{5 \\times 1}$. Portfolio volatility is estimated as: $w^{T}\\Omega w$ where $w \\in \\mathbb{R}^{5 \\times 1}$\n",
    "\n",
    "We consider the 3 major methods used in market risk management, specifically for the estimation of VaR. Please note that there are multiple different methods for estimating VaR and other more coherent risk measures such as Conditional Value-at-Risk (CVaR) however we are only considering the few major ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6010ead6-345d-4312-96d9-c61dcebb86d1",
   "metadata": {},
   "source": [
    "# Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6f07c3f-910f-4265-94f5-2c9264e98621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/woreom/anaconda3/envs/var/lib/python3.7/site-packages (1.21.0)\n",
      "Requirement already satisfied: pandas in /home/woreom/anaconda3/envs/var/lib/python3.7/site-packages (1.3.0)\n",
      "Requirement already satisfied: scipy in /home/woreom/anaconda3/envs/var/lib/python3.7/site-packages (1.7.0)\n",
      "Requirement already satisfied: matplotlib in /home/woreom/anaconda3/envs/var/lib/python3.7/site-packages (3.4.2)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/woreom/anaconda3/envs/var/lib/python3.7/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/woreom/anaconda3/envs/var/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/woreom/anaconda3/envs/var/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/woreom/anaconda3/envs/var/lib/python3.7/site-packages (from matplotlib) (8.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/woreom/anaconda3/envs/var/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/woreom/anaconda3/envs/var/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/woreom/anaconda3/envs/var/lib/python3.7/site-packages (from matplotlib) (1.3.1)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.0.1 scikit-learn-0.24.2 sklearn-0.0 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy pandas scipy matplotlib sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c07cca-b7c9-450b-ac60-4b0fa5af0057",
   "metadata": {},
   "source": [
    "## VaR: Variance-covariance method\n",
    "\n",
    "---\n",
    "\n",
    "The first one is the variance-covariance method and uses the estimated portfolio volatility $w^{T}\\Omega w$ under the Gaussian assumption to estimate VaR. Let's assume we are attempting to estimate 5% VaR: This means that there is a 5% probability of obtaining a portfolio return of less than the VaR value. Using the variance-covariance approach the calculation is: $\\left[\\left(w^{T}\\Omega w\\right) \\mathcal{N}^{-1}(1\\%)\\right] + w^{T}\\mu$, where $\\mu \\in \\mathbb{R}^{5 \\times 1}$ is the expected returns vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "454bb196-f3f9-45f5-bf55-7816da42c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries:Panda (reading data, manipulation, analysis), numpy(for mathematical array operations)\n",
    "\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35958500-6234-442c-ad5d-8a8fd60d0836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apple</th>\n",
       "      <th>Google</th>\n",
       "      <th>Microsoft</th>\n",
       "      <th>Intel</th>\n",
       "      <th>Box</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.025379</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.004552</td>\n",
       "      <td>-0.004718</td>\n",
       "      <td>-0.075180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.019764</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>-0.018332</td>\n",
       "      <td>-0.022419</td>\n",
       "      <td>-0.048452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.043121</td>\n",
       "      <td>-0.023443</td>\n",
       "      <td>-0.035402</td>\n",
       "      <td>-0.038206</td>\n",
       "      <td>-0.041840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.016546</td>\n",
       "      <td>0.003062</td>\n",
       "      <td>-0.010418</td>\n",
       "      <td>-0.047107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016063</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>0.017304</td>\n",
       "      <td>-0.009520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.001976</td>\n",
       "      <td>-0.001708</td>\n",
       "      <td>-0.004890</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.005096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.006331</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.010116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>-0.004273</td>\n",
       "      <td>-0.008246</td>\n",
       "      <td>-0.004593</td>\n",
       "      <td>-0.011940</td>\n",
       "      <td>-0.007215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>-0.000257</td>\n",
       "      <td>-0.002883</td>\n",
       "      <td>-0.001430</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.005776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>-0.007826</td>\n",
       "      <td>-0.014113</td>\n",
       "      <td>-0.012156</td>\n",
       "      <td>-0.010695</td>\n",
       "      <td>-0.002162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Apple    Google  Microsoft     Intel       Box\n",
       "0   -0.025379  0.000997   0.004552 -0.004718 -0.075180\n",
       "1   -0.019764  0.001400  -0.018332 -0.022419 -0.048452\n",
       "2   -0.043121 -0.023443  -0.035402 -0.038206 -0.041840\n",
       "3    0.005274 -0.016546   0.003062 -0.010418 -0.047107\n",
       "4    0.016063  0.002181  -0.000574  0.017304 -0.009520\n",
       "..        ...       ...        ...       ...       ...\n",
       "246  0.001976 -0.001708  -0.004890  0.001083  0.005096\n",
       "247  0.006331  0.002074   0.000632  0.002701  0.010116\n",
       "248 -0.004273 -0.008246  -0.004593 -0.011940 -0.007215\n",
       "249 -0.000257 -0.002883  -0.001430  0.000819  0.005776\n",
       "250 -0.007826 -0.014113  -0.012156 -0.010695 -0.002162\n",
       "\n",
       "[251 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I indicated an arbirary weight allocation to portfolio stocks\n",
    "weights = np.array([0.166684,0.300577,0.258903,0.227786,0.046049])\n",
    "\n",
    "# I set the initial investment amount\n",
    "initial_investment = 2500000\n",
    "\n",
    "# Opening data\n",
    "df = pd.read_csv(\"ret_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c23fd550-ee9b-440d-866d-44f8b3c1cca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apple</th>\n",
       "      <th>Google</th>\n",
       "      <th>Microsoft</th>\n",
       "      <th>Intel</th>\n",
       "      <th>Box</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>251.000000</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>251.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>-0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.014773</td>\n",
       "      <td>0.012576</td>\n",
       "      <td>0.014316</td>\n",
       "      <td>0.014426</td>\n",
       "      <td>0.027995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.067965</td>\n",
       "      <td>-0.054645</td>\n",
       "      <td>-0.074411</td>\n",
       "      <td>-0.095432</td>\n",
       "      <td>-0.125227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.005969</td>\n",
       "      <td>-0.004931</td>\n",
       "      <td>-0.006021</td>\n",
       "      <td>-0.006334</td>\n",
       "      <td>-0.011028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.001599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.007676</td>\n",
       "      <td>0.007688</td>\n",
       "      <td>0.007161</td>\n",
       "      <td>0.008498</td>\n",
       "      <td>0.015230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.062940</td>\n",
       "      <td>0.043293</td>\n",
       "      <td>0.056571</td>\n",
       "      <td>0.034435</td>\n",
       "      <td>0.082065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Apple      Google   Microsoft       Intel         Box\n",
       "count  251.000000  251.000000  251.000000  251.000000  251.000000\n",
       "mean     0.000377    0.000158    0.000501    0.000259   -0.000141\n",
       "std      0.014773    0.012576    0.014316    0.014426    0.027995\n",
       "min     -0.067965   -0.054645   -0.074411   -0.095432   -0.125227\n",
       "25%     -0.005969   -0.004931   -0.006021   -0.006334   -0.011028\n",
       "50%      0.000756    0.000217    0.000356    0.001343    0.001599\n",
       "75%      0.007676    0.007688    0.007161    0.008498    0.015230\n",
       "max      0.062940    0.043293    0.056571    0.034435    0.082065"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To explore some key statistics:\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ee6f790-3092-4742-9cbe-c3393f8c5adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apple</th>\n",
       "      <th>Google</th>\n",
       "      <th>Microsoft</th>\n",
       "      <th>Intel</th>\n",
       "      <th>Box</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Apple</th>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Google</th>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Microsoft</th>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intel</th>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Box</th>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Apple    Google  Microsoft     Intel       Box\n",
       "Apple      0.000218  0.000088   0.000104  0.000097  0.000150\n",
       "Google     0.000088  0.000158   0.000126  0.000089  0.000085\n",
       "Microsoft  0.000104  0.000126   0.000205  0.000124  0.000136\n",
       "Intel      0.000097  0.000089   0.000124  0.000208  0.000155\n",
       "Box        0.000150  0.000085   0.000136  0.000155  0.000784"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_matrix= df.cov()\n",
    "\n",
    "cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c050fe83-b823-4869-9f7f-404e3485fbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.10884557e-05, 9.73317458e-05, 1.23219532e-04, 3.53496287e-04,\n",
       "       9.48065819e-04])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eigenvalues of covariance matrixe\n",
    "np.linalg.eigvalsh(cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b50a273-e359-4ca9-bba5-01a2ae8ae1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple        0.000377\n",
       "Google       0.000158\n",
       "Microsoft    0.000501\n",
       "Intel        0.000259\n",
       "Box         -0.000141\n",
       "dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the mean returns for each stock\n",
    "\n",
    "average_returns= df.mean()\n",
    "\n",
    "average_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87490373-3ae5-4121-825d-0eb89b553f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00029243952798192134"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Overall portfolio average returns,using the dot product formular for the means against investment weights\n",
    "\n",
    "Portf_mean =average_returns. dot(weights)\n",
    "\n",
    "Portf_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa7ac458-b346-4921-9c73-16dcaee15690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0114"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Portfolio Standard Deviation is calculated using :\n",
    "\n",
    "Portf_stdev= round(np.sqrt(weights.T.dot(cov_matrix).dot(weights)),4)\n",
    "\n",
    "Portf_stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "788cf20a-92c5-477c-8edf-8889033aafd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500731.0988"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the mean value of the given investment\n",
    "\n",
    "Mean_investment = round((1+ Portf_mean)*initial_investment,4)\n",
    "\n",
    "Mean_investment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "023aaf85-150c-408c-a033-c7f3441d8d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28500.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard deviation of the investment\n",
    "\n",
    "Stdev_investment = initial_investment* Portf_stdev\n",
    "\n",
    "Stdev_investment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef5e00aa-589a-40da-a454-7dd75f570781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I then proceed to choose the confidence interval of interest. say 95  per cent\n",
    "\n",
    "Conf_level1 = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3cf470f-f67e-4a03-a253-41699a314793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2453852.7704318827"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the Scipy percentage point function(ppf) method to generate values for the inverse cummulative density function \n",
    "#to a normal distribution, I plugged in the mean and stdev of the portfolio calculated earlier\n",
    "\n",
    "#It takes a percentage and returns a standard deviation multiplier for what value that percentage occurs at\n",
    "# For instance, norm.ppf(0.90, loc= 134, scale =3.45) will return a value(that functions as standard-deviation multiplier) marking\n",
    "# where 95% of data point would be contained if our data follows a normal distribution. \n",
    "\n",
    "Cutoff1 =norm.ppf(Conf_level1, Mean_investment, Stdev_investment)\n",
    "\n",
    "Cutoff1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17bbda98-0881-4664-921f-23dce3db7a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46147.23"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eventually, I am able to calculate the VaR at the confidence interval chosen. \n",
    "# Interpretation: By this result, we are saying that the loss in our portfolio will not exceed $46148 over 1-day period @95 C.I\n",
    "\n",
    "VaR_1dl = round(initial_investment- Cutoff1,3)\n",
    "\n",
    "VaR_1dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f297ea7-3206-4f31-b5f8-dd01340c3d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 day VaR @ 95% Confidence: 46147.23\n",
      "2 day VaR @ 95% Confidence: 65262.04\n",
      "3 day VaR @ 95% Confidence: 79929.35\n",
      "4 day VaR @ 95% Confidence: 92294.46\n",
      "5 day VaR @ 95% Confidence: 103188.34\n"
     ]
    }
   ],
   "source": [
    "# What if we decided to evaluate this over a bigger window of time? \n",
    "# In this case,we take 1-day VaR multiplied by square root of the select time period\n",
    "# That is, in order to calculate n-Day VaR\n",
    "# Observation:  As the the time window is expanded as below, the potential loss tends to increase. This seems logical\n",
    "\n",
    "num_days = 5\n",
    "\n",
    "for x in range(1, num_days+1):\n",
    "     print(str(x) + \" day VaR @ 95% Confidence: \" + str(np.round(VaR_1dl * np.sqrt(x),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c2e08f-1164-4e86-a3de-98a28c76d9d4",
   "metadata": {},
   "source": [
    "## VaR: Historical simulation method\n",
    "The second method is a non-parametric approach where we sample with replacement from the historical data to estimate a portfolio returns distribution. The 5% VaR is simply the appropriate quantile from this sampled portfolio returns distribution.\n",
    "\n",
    "Historical Simulation (HS) VaR is instead efficient when the risk manager cannot, or doesn’t intend to, make assumptions on the underlying distribution of returns as it is calculated by the simple picking of the chosen percentile loss in a given period of time.\n",
    "\n",
    "This method is even simpler than the parametric one & that is precisely its weakness. The main underlying logic is that the past is a good predictor for the future. By looking back, it is then possible to incorporate all data points into the risk calculation but this unfortunately leads to too simplistic a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0d5e9c07-617a-4225-aed7-cf7d6a27d674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12313456457505044"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Confidence_Interval=0.95\n",
    "\n",
    "Value_at_Risk = -np.percentile(df,1-Confidence_Interval)\n",
    "Value_at_Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ac4c1-6975-4de2-a956-78d836b6a184",
   "metadata": {},
   "source": [
    "## VaR: Monte Carlo method\n",
    "\n",
    "---\n",
    "\n",
    "The third method is Monte Carlo sampling from a multidimensional Gaussian distribution using the aforementioned $mu$ and $\\Omega$ parameters. Finally the 5% VaR is simply the appropriate quantile from this sampled portfolio returns distribution.\n",
    "\n",
    "In general, MCS is a technique that \"converts\" uncertainty on input variables of a model into **probability distributions**. By combining the distributions and randomly selecting values from them, it recalculates the simulated model many times, to determine the probability of the output.\n",
    "\n",
    "Historically, this technique was first used by scientists working on the atomic bomb: it was named after Monte Carlo, the Monaco resort town renowned for its casinos.  Since its introduction in World War II, Monte Carlo simulation has been used to model a variety of physical and conceptual systems.\n",
    "\n",
    "### How does it work?\n",
    "Monte Carlo simulation performs risk analysis by building models of possible results by *substituting a range of possible input values, that constitute uncertainty, into a statistical distribution*. It then computes possible outcomes repeatedly, each time using a different set of random values from the probability functions that \"model\" the input. Depending upon the number of random input variables and their distribution, a Monte Carlo simulation could involve thousands or tens of thousands of \"rounds\" before it is complete. When complete, *Monte Carlo simulation produces distributions of possible outcome values*.\n",
    "\n",
    "By using probability distributions instead of actual input samples, it is possible to model more accurately uncertainty: different choices of distributions will yield different outputs.\n",
    "\n",
    "### A brief summary of the Monte Carlo Simulation (MCS) technique\n",
    "\n",
    "- A MCS allows several inputs to be used at the same time to compute the probability distribution of one or more outputs\n",
    "- Different types of probability distributions can be assigned to the inputs of the model, depending on any *a-priori* information that is available. When the distribution is completely unknown, a common technique is to use a distribution computed by finding the best fit to the data you have\n",
    "- The MCS method is also called a **stochastic method** because it uses random variables. Note also that the general assumption is for input random variables to be independent from each other. When this is not the case, there are techniques to account for correlation between random variables.\n",
    "- A MCS generates the output as a range instead of a fixed value and shows how likely the output value is to occur in that range. In other words, the model outputs a probability distribution.\n",
    "\n",
    "### Common distributions used in MCS\n",
    "In what follows, we summarize the most common probability distributions that are used as *a-priori* distributions for input random variables:\n",
    "\n",
    "- *Normal/Gaussian Distribution*: this is a continuous distribution applied in situations where the mean and the standard deviation of a given input variable are given, and the mean represents the most probable value of the variable. In other words, values \"near\" the mean are most likely to occur.  This is symmetric distribution, and it is not bounded in its co-domain. It is very often used to  describe natural phenomena, such as people’s heights, inflation rates, energy prices, and so on and so forth. An illustration of a normal distribution is given below:\n",
    "![normal_distribution](https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/320px-Normal_Distribution_PDF.svg.png)\n",
    "\n",
    "- *Lognormal Distribution*: this is a distribution which is appropriate for variables taking values in the range $[0, \\infty]$. Values are positively skewed, not symmetric like a normal distribution.  Examples of variables described by some lognormal distributions include, for example, real estate property values, stock prices, and oil reserves. An illustration of a lognormal distribution is given below:\n",
    "![log_normal_distribution](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/PDF-log_normal_distributions.svg/320px-PDF-log_normal_distributions.svg.png) \n",
    "\n",
    "- *Triangular Distribution*: this is a continuous distribution with fixed minimum and maximum values. It is bounded by the minimum and maximum values and can be either symmetrical (the most probable value = mean = median) or asymmetrical. Values around the most likely value (e.g. the mean) are more likely to occur.  Variables that could be described by a triangular distribution include, for example, past sales history per unit of time and inventory levels. An illustration of a triangular distribution is given below:\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Triangular_distribution_PMF.png/320px-Triangular_distribution_PMF.png)\n",
    "\n",
    "- *Uniform Distribution*: this is a continuous distribution bounded by known minimum and maximum values. In contrast to the triangular distribution, the likelihood of occurrence of the values between the minimum and maximum is the same. In other words, all values have an equal chance of occurring, and the distribution is simply characterized by the minimum and maximum values. Examples of variables that can be described by a uniform distribution include manufacturing costs or future sales revenues for a new product. An illustration of the uniform distribution is given below:\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/96/Uniform_Distribution_PDF_SVG.svg/320px-Uniform_Distribution_PDF_SVG.svg.png)\n",
    "\n",
    "- *Exponential Distribution*: this is a continuous distribution used to model the time that pass between independent occurrences, provided that the rate of occurrences is known. An example of the exponential distribution is given below:\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Exponential_pdf.svg/320px-Exponential_pdf.svg.png)\n",
    "\n",
    "- *Discrete Distribution* : for this kind of distribution, the \"user\" defines specific values that may occur and the likelihood of each of them.  An example might be the results of a lawsuit: 20% chance of positive verdict, 30% change of negative verdict, 40% chance of settlement, and 10% chance of mistrial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c15370-9c9c-4c0a-b28b-9f91e50951ca",
   "metadata": {},
   "source": [
    "### Pseudo-Code for using Monte Carlo to calculate VaR and Expected Shortfall (ES)\n",
    "\n",
    "* $n=5$ is the number of assets.\n",
    "* $m=5000$ is the number of samples/simulations I carried out.\n",
    "* $T=10$ is the $10$ days time horizon.\n",
    "* $M=251$ be the number of days I look back in for each asset to compute mean and std. \n",
    "* Let $S_i(0)$ be the today's price of the $i^{th}$ asset in the portfolio where $i\\in\\{1,\\dots,n\\}$.\n",
    "* Let $c_i = 100$ be the number of contracts of $i^{th}$ asset we have in our portfolio. In our example, they are all identical but in reality, they can be different depending on your position. \n",
    "* Let $P_p(0) = \\sum_{i=1}^{n} c_iS_i(0)$ be the initial portfolio price.\n",
    "* Let $w = \\{w_i\\}_{i\\in\\{1,\\dots,n\\}}\\in\\mathbb{R}^n$ be the weight of EACH asset of the portfolio where $w_i = \\frac{c_iS_i(0)}{P_p(0)}$ for $i\\in\\{1,\\dots,n\\}$.\n",
    "* Let $H\\in\\mathbb{R}^{n\\times M}$ be a matrix storing the HISTORICAL data of $M$ days and $n$ assets. \n",
    "* Let $$R = \\frac{H[:,1:]}{H[:,:-1]} - 1 \\in \\mathbb{R}^{n\\times (M-1)}$$ be a $n$ by $M-1$ RETURNS matrix storing all the returns for $n$ assets where $R_{ij} = \\frac{H_{i,j+1}}{H_{ij}}- 1$ and $i\\in\\{1,\\dots,n\\}$, $j\\in\\{1,\\dots,M-1\\}$.\n",
    "* Let $\\mu\\in\\mathbb{R}^n$ be the return mean vector for each asset where $$\\mu_i = \\frac{\\sum_{j=1}^{M-1} R_{ij}}{M-1}$$. Note, here I have computed $\\textbf{Equally weighted mean}$ but I think it is better to give a higher weightage to more recent return values. Hence, we using $\\textbf{Exponentially weighted mean}$ is a good idea which I omitted for simplicity. \n",
    "* Let $\\Sigma \\in \\mathbb{R^{n\\times n}}$ be the correlation matrix of the returns of assets in the portfolio.\n",
    "* Let $D = \\sqrt{diag(\\Sigma)}\\in \\mathbb{R^{n\\times n}}$ be the the diagonal matrix storing standard deviation of each asset where $D_{ii} = \\sqrt{\\Sigma_{ii}} = \\sigma_{i}$.\n",
    "* Let: \n",
    "$$\\Sigma = LL^T$$ be a __Cholesky Decomposition__ of correlation matrix and $L$ is a __lower triangular matrix__. \n",
    "\n",
    "* $\\textbf{For $k = 1:m$}$:  $\\quad \\quad(\\textit{OUTER loop}$)\n",
    "    - Take $prod^{(k,0)} = \\mathbb{1}\\in\\mathbb{R}^n$ is a $n$ dimensional vector of just ones. I need it in the future. \n",
    "    \n",
    "    - $\\textbf{For $j = 1:T$}$: $\\quad \\quad(\\textit{INNER loop}$)\n",
    "        * Let $X^{(k,j)}\\in\\mathbb{R}^{n}$ be __independent standard normal vector__ i.e. each $X^{(k,j)}_i\\sim N(0,1)$ and $Cov(X^{(k,j)}_i,X^{(k,j)}_p) = 0$ where $i\\neq p$. \n",
    "        * Let $Y^{(k,j)} = LX^{(k,j)}$ be __CORRELATED standard normal vector__ i.e. each $Y^{(k,j)}_i\\sim N(0,1)$ and $Cov(Y^{(k,j)}_i,Y^{(k,j)}_p)\\neq 0$ necessarily. \n",
    "        * $r^{(k,j)} = \\mu + DY^{(k,j)} $ is a $n$ dimensional return vector for storing the return of EACH asset. I have $\\textbf{assumed that returns of EACH asset are normally distributed with mean}$ $\\mu_i$ and variance $\\sigma_i^2$ i.e $r^{(k,j)}_i\\sim N(\\mu_i, \\sigma_i^2)$ and $r^{(k,j)}_i = \\mu_i + \\sigma_iY^{(k,j)}_i$ for $i\\in\\{1,\\dots,n\\}$.  \n",
    "        * $prod^{(k,j+1)} = prod^{(k,j)}*(\\mathbb{1}+r^{(k,j)})$ is a bit like you are stepping through the time $1$ to $T$ and computing the returns of EACH asset by random simulations which is then compounded. \n",
    "    - Now we step out of the INNER loop and compute the __portfolio return at the $k^{th}$ outer iteration__: \n",
    "    $$P^{(k)}_r = \\sum_{i=1}^{n} w_i*prod^{(k,T)}_i \\in\\mathbb{R}$$.\n",
    "* Now we step out of the OUTER loop, and take: \n",
    "$$\\textbf{P}_r = \\{P_r^{(1)},\\dots,P_r^{(m)}\\}\\in\\mathbb{R}^m$$ be a __$m$ dimensional vector of all the portfolio returns simulated above__. \n",
    "* $\\text{Loss_vec} = P_p(0)[\\textbf{P}_r - \\mathbb{1}] \\in \\mathbb{R}^m$ stores how the __price of the portfolio__ i.e. $P_p(T)$ have changed for today's price to the time horizon $T=10$. \n",
    "* SORT the $\\text{Loss_vec}$ vector in the ascending order such that:\n",
    "$$\\tilde{P}_r^{(1)} \\leq \\dots \\leq \\tilde{P}_r^{(m)}$$\n",
    "* Take $\\alpha = 0.01$ where $(1-\\alpha)$ be the confidence interval to which we are computing our VaR to. \n",
    "* Take $idx = \\lceil m\\alpha \\rceil$ which is the index at which we have our VaR value. I just use the $\\textbf{ceiling}$ fucntion but you can also $\\textbf{interpolate}$ between the indices to get a better estimation of the VaR. \n",
    "* $VaR = -\\tilde{P}_r^{(idx)}$ is the computation of the value at risk at $(1-\\alpha)$ confidence interval. \n",
    "* $ES = \\frac{-1}{idx}\\sum_{k=1}^{idx} \\tilde{P}_r^{(k)}$ is the expected shortfall which is basically the average of all the returns below and including the VaR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "59f66aec-d110-4ce0-9324-e159e8c138e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple        0.000377\n",
       "Google       0.000158\n",
       "Microsoft    0.000501\n",
       "Intel        0.000259\n",
       "Box         -0.000141\n",
       "dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Here I am setting up the problem by importing all the historic data, \n",
    "    computing the mean/std of asset returns and also computing the \n",
    "    cholesky decomposition of the correlation matrix etc.'''\n",
    "\n",
    "no_of_assets = 5; #number of assets\n",
    "no_of_sims = 5000; #number of simulations.\n",
    "time_horizon = 10; #time horizon which we are trying to forecast the VaR\n",
    "go_back_days = 251 # number of days to look at the data in the past to compute the mean/std of asset's returns\n",
    "alpha = 0.05  # (1-alpha) is the confidence interval we are computing VaR to\n",
    "\n",
    "tickers = [\"Apple\",\"Google\",\"Microsoft\",\"Intel\",\"Box\"]  #tickers of the assets in the portfolio\n",
    "no_of_contracts = [100,100,100,100,100] #number of contracts for EACH asset in the portfolio\n",
    "\n",
    "# I set the initial investment amount\n",
    "initial_investment = 2500000\n",
    "\n",
    "# Opening data\n",
    "df = pd.read_csv(\"ret_data.csv\")\n",
    "df\n",
    "\n",
    "initial_asset_prices = df.iloc[-1] #today prices are the last column\n",
    "initial_portfolio_price = no_of_contracts@initial_asset_prices #initial portfolio price\n",
    "weights = no_of_contracts*initial_asset_prices/initial_portfolio_price\n",
    "\n",
    "mean_vec=df.mean()\n",
    "mean_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e87a46e-453f-439e-ac57-ab038419294a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple        0.166684\n",
       "Google       0.300577\n",
       "Microsoft    0.258903\n",
       "Intel        0.227786\n",
       "Box          0.046049\n",
       "Name: 250, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60516e64-4585-4b90-93f4-0478b0ce5523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple        0.014773\n",
       "Google       0.012576\n",
       "Microsoft    0.014316\n",
       "Intel        0.014426\n",
       "Box          0.027995\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_vec=df.std() #computing the standard deviation\n",
    "std_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a709333-ed45-4af1-aada-f64f3c8decda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Apple    Google  Microsoft     Intel       Box\n",
      "Apple      1.000000  0.475370   0.489525  0.456646  0.363502\n",
      "Google     0.475370  1.000000   0.699470  0.489599  0.240516\n",
      "Microsoft  0.489525  0.699470   1.000000  0.600531  0.338984\n",
      "Intel      0.456646  0.489599   0.600531  1.000000  0.384221\n",
      "Box        0.363502  0.240516   0.338984  0.384221  1.000000\n"
     ]
    }
   ],
   "source": [
    "correlation_mat = df.corr()  #computing the correlation matrix\n",
    "print(correlation_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7972feba-23bd-4057-b3d3-820e50acacd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27777327, 0.481286  , 0.56685801, 0.8238901 , 2.85019261])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eigenvalues of correlation matrixe\n",
    "np.linalg.eigvalsh(correlation_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6431335a-5d7d-4ac6-9f74-c83d18b82508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.47536993, 0.87978601, 0.        , 0.        , 0.        ],\n",
       "       [0.4895251 , 0.53054297, 0.69201831, 0.        , 0.        ],\n",
       "       [0.45664608, 0.30976126, 0.30728922, 0.77530359, 0.        ],\n",
       "       [0.36350176, 0.07697136, 0.17370091, 0.18187691, 0.89369495]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cholesky of the correlation matrix\n",
    "cholesky_mat = np.linalg.cholesky(correlation_mat) \n",
    "cholesky_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bccabbf2-c77f-4365-b087-1c3985cd18dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Weights of EACH asset in the portfolio: Apple        0.166684\n",
      "Google       0.300577\n",
      "Microsoft    0.258903\n",
      "Intel        0.227786\n",
      "Box          0.046049\n",
      "Name: 250, dtype: float64\n",
      "- Initial Price of portfolio: $ -4.6953169280287295\n",
      "- Returns mean for EACH asset: Apple        0.000377\n",
      "Google       0.000158\n",
      "Microsoft    0.000501\n",
      "Intel        0.000259\n",
      "Box         -0.000141\n",
      "dtype: float64\n",
      "- Returns standard deviation for EACH asset: Apple        0.014773\n",
      "Google       0.012576\n",
      "Microsoft    0.014316\n",
      "Intel        0.014426\n",
      "Box          0.027995\n",
      "dtype: float64\n",
      "\n",
      "- Correlation Matrix:\n",
      "              Apple    Google  Microsoft     Intel       Box\n",
      "Apple      1.000000  0.475370   0.489525  0.456646  0.363502\n",
      "Google     0.475370  1.000000   0.699470  0.489599  0.240516\n",
      "Microsoft  0.489525  0.699470   1.000000  0.600531  0.338984\n",
      "Intel      0.456646  0.489599   0.600531  1.000000  0.384221\n",
      "Box        0.363502  0.240516   0.338984  0.384221  1.000000\n",
      "\n",
      "- Cholesky of correlation Matrix:\n",
      "[[1.         0.         0.         0.         0.        ]\n",
      " [0.47536993 0.87978601 0.         0.         0.        ]\n",
      " [0.4895251  0.53054297 0.69201831 0.         0.        ]\n",
      " [0.45664608 0.30976126 0.30728922 0.77530359 0.        ]\n",
      " [0.36350176 0.07697136 0.17370091 0.18187691 0.89369495]]\n",
      "\n",
      "- 10 days VaR with 95.0% CI: $ 0.28736715359189263\n",
      "- 10 days Expected Shortfall (ES) with 95.0% CI: $ 0.3630979906813018\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def mc_VaR_standard_way(printing=False):\n",
    "    '''This is a the code for my pseudo-code above'''\n",
    "    portfolio_returns_sims = np.zeros(no_of_sims) #creating an empty zero array\n",
    "\n",
    "    for k in range(no_of_sims):\n",
    "        prod_k = np.ones(no_of_assets) #vector of ones of no_of_assets dims\n",
    "        np.random.seed(k) #fixed a seed \n",
    "        for j in range(time_horizon):\n",
    "            X_kj = np.random.normal(loc = 0.0, scale = 1.0, size=no_of_assets) #generating standard normal sims\n",
    "            Y_kj = cholesky_mat@X_kj  #correlating them with each other\n",
    "\n",
    "            # making an assumption that returns of each asset is normally distributed \n",
    "            # with mean mu_i and standard deviation simga_i, hence I unstandarise it.\n",
    "            r_kj = mean_vec + std_vec*Y_kj   \n",
    "            prod_k = prod_k*(1+r_kj)  #a bit like I am stepping through the time horizon and doing compound interest\n",
    "        portfolio_returns_sims[k] = weights@prod_k  #simulated returns of the portfolio in the kth sim\n",
    "\n",
    "    loss_vector = initial_portfolio_price*(portfolio_returns_sims - 1) #change in the portfolio price at t= 0  and t=T\n",
    "    sorted_loss_vector = np.sort(loss_vector) #sorting the loss vector\n",
    "    #finding the index where we have VaR value\n",
    "    #I just took the ceiling function here but feel free to interpolate here\n",
    "    index = int(np.ceil(alpha*no_of_sims)) - 1 \n",
    "    full_VaR = sorted_loss_vector[index]  #computing the VaR \n",
    "    ES = sum(sorted_loss_vector[:index+1])/(index+1) #computing the Expected Shortfall (ES)\n",
    "\n",
    "    if printing:\n",
    "        print(f'\\n- Weights of EACH asset in the portfolio: {weights}')\n",
    "        print(f'- Initial Price of portfolio: $ {initial_portfolio_price}')\n",
    "        print(f'- Returns mean for EACH asset: {mean_vec}')\n",
    "        print(f'- Returns standard deviation for EACH asset: {std_vec}\\n')\n",
    "        print(f'- Correlation Matrix:\\n{correlation_mat}\\n')\n",
    "        print(f'- Cholesky of correlation Matrix:\\n{cholesky_mat}\\n')\n",
    "        print(f'- {time_horizon} days VaR with {(1-alpha)*100}% CI: $ {-full_VaR}')\n",
    "        print(f'- {time_horizon} days Expected Shortfall (ES) with {(1-alpha)*100}% CI: $ {-ES}')\n",
    "    \n",
    "    return full_VaR\n",
    "\n",
    "full_Var = mc_VaR_standard_way(printing=True)\n",
    "print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3d5925-354e-40a6-b8d9-438dd7147b7a",
   "metadata": {},
   "source": [
    "### Simpler Case Monte Carlo: Computing one day VaR and then extending it to T days by multiplying by $\\sqrt{T}$\n",
    "\n",
    "This is also the MC method, but I have assumed that returns of EACH assets are I.I.D with respect to EACH day. Hence, scaling by a factor of $\\sqrt{T}$ (as Brownian Motion is of the order $O(\\sqrt{T}))$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6683a317-f132-4feb-b4d7-c97f54fedc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Weights of EACH asset in the portfolio: Apple        0.166684\n",
      "Google       0.300577\n",
      "Microsoft    0.258903\n",
      "Intel        0.227786\n",
      "Box          0.046049\n",
      "Name: 250, dtype: float64\n",
      "- Initial Price of portfolio: $ -4.6953169280287295\n",
      "- Returns mean for EACH asset: Apple        0.000377\n",
      "Google       0.000158\n",
      "Microsoft    0.000501\n",
      "Intel        0.000259\n",
      "Box         -0.000141\n",
      "dtype: float64\n",
      "- Returns standard deviation for EACH asset: Apple        0.014773\n",
      "Google       0.012576\n",
      "Microsoft    0.014316\n",
      "Intel        0.014426\n",
      "Box          0.027995\n",
      "dtype: float64\n",
      "\n",
      "- Correlation Matrix:\n",
      "              Apple    Google  Microsoft     Intel       Box\n",
      "Apple      1.000000  0.475370   0.489525  0.456646  0.363502\n",
      "Google     0.475370  1.000000   0.699470  0.489599  0.240516\n",
      "Microsoft  0.489525  0.699470   1.000000  0.600531  0.338984\n",
      "Intel      0.456646  0.489599   0.600531  1.000000  0.384221\n",
      "Box        0.363502  0.240516   0.338984  0.384221  1.000000\n",
      "\n",
      "- Cholesky of correlation Matrix:\n",
      "[[1.         0.         0.         0.         0.        ]\n",
      " [0.47536993 0.87978601 0.         0.         0.        ]\n",
      " [0.4895251  0.53054297 0.69201831 0.         0.        ]\n",
      " [0.45664608 0.30976126 0.30728922 0.77530359 0.        ]\n",
      " [0.36350176 0.07697136 0.17370091 0.18187691 0.89369495]]\n",
      "\n",
      "- 10 days VaR with 95.0% CI: $ 0.2779121256741909\n",
      "- 10 days Expected Shortfall (ES) with 95.0% CI: $ 0.3497249307283379\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def mc_VaR_simpler(printing=False):\n",
    "    portfolio_returns_sims = np.zeros(no_of_sims) #creating an empty zero array\n",
    "\n",
    "    for k in range(no_of_sims):\n",
    "        np.random.seed(k) #fixed a seed \n",
    "        X_k = np.random.normal(loc = 0.0, scale = 1.0, size=no_of_assets) #generating standard normal sims\n",
    "        Y_k = cholesky_mat@X_k  #correlating them with each other and Y_k is also standard normal.\n",
    "\n",
    "        #making an assumption that returns of each asset is normally distributed \n",
    "        # with mean mu_i and standard deviation sigma_i, hence I unstandarise it\n",
    "        r_k = mean_vec + std_vec*Y_k   \n",
    "        portfolio_returns_sims[k] = weights@(1+r_k)  #simulated returns of portfolio in the kth sim\n",
    "\n",
    "    loss_vector = initial_portfolio_price*(portfolio_returns_sims-1)\n",
    "    sorted_loss_vector = np.sort(loss_vector)\n",
    "    index = int(np.ceil(alpha*no_of_sims)) - 1 #I just took the ceiling function here but feel free to interpolate here\n",
    "    one_day_VaR = sorted_loss_vector[index] #computing the one-day-VaR here\n",
    "    full_VaR = one_day_VaR*np.sqrt(time_horizon) #extending to a T day VaR\n",
    "\n",
    "    ES = np.sqrt(time_horizon)*sum(sorted_loss_vector[:index+1])/(index+1) #expected shortfall\n",
    "    \n",
    "    if printing:\n",
    "        print(f'\\n- Weights of EACH asset in the portfolio: {weights}')\n",
    "        print(f'- Initial Price of portfolio: $ {initial_portfolio_price}')\n",
    "        print(f'- Returns mean for EACH asset: {mean_vec}')\n",
    "        print(f'- Returns standard deviation for EACH asset: {std_vec}\\n')\n",
    "        print(f'- Correlation Matrix:\\n{correlation_mat}\\n')\n",
    "        print(f'- Cholesky of correlation Matrix:\\n{cholesky_mat}\\n')\n",
    "        print(f'- {time_horizon} days VaR with {(1-alpha)*100}% CI: $ {-full_VaR}')\n",
    "        print(f'- {time_horizon} days Expected Shortfall (ES) with {(1-alpha)*100}% CI: $ {-ES}')\n",
    "        \n",
    "    return full_VaR, one_day_VaR\n",
    "        \n",
    "full_Var = mc_VaR_simpler(printing=True)\n",
    "print('='*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VaR",
   "language": "python",
   "name": "var"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
